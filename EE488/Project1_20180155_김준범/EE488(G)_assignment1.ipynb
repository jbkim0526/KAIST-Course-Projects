{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EE488(G) assignment1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxXHnv3PSH7Y"
      },
      "source": [
        "# **EE488 Machine Learning basics and practice**\n",
        "## **Mini-Assignment 1**\n",
        "\n",
        "## Objective\n",
        "* part 1. Data Preparation\n",
        "* part 2. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gddbFLmzTIwY"
      },
      "source": [
        "## **Part1** Data Preparation\n",
        "* This part is for downloading the dataset to be used in this project. \n",
        "* All the related codes are basically provided below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YisgO_4MYYev"
      },
      "source": [
        "### Step1. Download and decompress the MNIST dataset (4, 9 classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiZxXnxtDguu"
      },
      "source": [
        "#Setting\n",
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "import os \n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from urllib import request\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I6pzWTeXX9Y"
      },
      "source": [
        "# {4, 9} class mnist dataset download and decompress\n",
        "!pip install gdown\n",
        "!gdown --id 1n70aILNV1U8I2VJlYekKuhBGbrNtCUNb\n",
        "!tar -xvf mnist_49.tar\n",
        "!rm -f mnist_49.tar\n",
        "\n",
        "# Dataset Load\n",
        "X_train = np.load('mnist_49/X_train.npy')\n",
        "X_test = np.load('mnist_49/X_test.npy')\n",
        "Y_train = np.load('mnist_49/Y_train.npy')\n",
        "Y_test = np.load('mnist_49/Y_test.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvjTVTMhHY8A"
      },
      "source": [
        "# Reshape the training and test examples\n",
        "# Data from Part 1 - Step 1\n",
        "X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "print('X_train_flatten shape: ' + str(X_train_flatten.shape))\n",
        "print('Y_train shape: ' + str(Y_train.shape))\n",
        "print('X_test_flatten shape: ' + str(X_test_flatten.shape))\n",
        "print('Y_test shape: ' + str(Y_test.shape))\n",
        "\n",
        "X_train_std = X_train_flatten / 255.\n",
        "X_test_std = X_test_flatten / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aPMCIAdYMLC"
      },
      "source": [
        "### Step2. Download and define a function for preparing the CIFAR10 dataset\n",
        "If you put the classes you want to use in the form of a list (ex, [0, 1], [3, 5, 8], etc.) as an argument to the *'cifar10_subset'* function, then, this function returns training data/label and test data/label of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPhUWoshDP7g"
      },
      "source": [
        "#Download CIFAR10 Dataset\n",
        "request.urlretrieve('https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz', 'cifar-10-python.tar.gz')\n",
        "!tar -xzf cifar-10-python.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwDRUSFPFnPZ"
      },
      "source": [
        "for i in range(1,6):\n",
        "    filename='data_batch_%d'%i\n",
        "    with open('cifar-10-batches-py/'+filename, 'rb') as fo:\n",
        "        dict_data = pickle.load(fo, encoding='bytes')\n",
        "  \n",
        "    images_batch = dict_data[b'data']\n",
        "    labels_batch = dict_data[b'labels']\n",
        "    if i==1:\n",
        "        train_images = images_batch\n",
        "        train_labels = labels_batch\n",
        "    else:\n",
        "        train_images = np.concatenate((train_images, images_batch), axis=0)\n",
        "        train_labels.extend(labels_batch)\n",
        "        \n",
        "with open('cifar-10-batches-py/test_batch', 'rb') as fo:\n",
        "    dict_data = pickle.load(fo, encoding='bytes')\n",
        "\n",
        "test_images = dict_data[b'data']\n",
        "test_labels = dict_data[b'labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B08BhNp8GHCT"
      },
      "source": [
        "train_indexes = sorted(range(len(train_labels)), key=lambda k: train_labels[k])\n",
        "test_indexes = sorted(range(len(test_labels)), key=lambda k: test_labels[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUU3YBhM_rVL"
      },
      "source": [
        "def cifar10_subset(class_ind=[0,1]):\n",
        "    assert (len(class_ind) >=2 and len(class_ind) <=10)\n",
        "    assert (min(class_ind) >=0 and max(class_ind) <=9)\n",
        "    train_images_cl = []\n",
        "    train_labels_cl = []\n",
        "    test_images_cl = []\n",
        "    test_labels_cl = []\n",
        "    for i in class_ind:\n",
        "        train_inds = train_indexes[i*5000:(i+1)*5000]\n",
        "        test_inds = test_indexes[i*1000:(i+1)*1000]\n",
        "        train_images_tmp = train_images[train_inds,:]\n",
        "        train_labels_tmp = np.array(train_labels)[train_inds]\n",
        "        test_images_tmp = test_images[test_inds,:]\n",
        "        test_labels_tmp = np.array(test_labels)[test_inds]\n",
        "\n",
        "        train_images_cl.append(train_images_tmp)\n",
        "        train_labels_cl.append(train_labels_tmp)\n",
        "        test_images_cl.append(test_images_tmp)\n",
        "        test_labels_cl.append(test_labels_tmp)\n",
        "  \n",
        "    return np.array(train_images_cl), np.array(train_labels_cl), np.array(test_images_cl), np.array(test_labels_cl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqkcC9fRX26Q"
      },
      "source": [
        "## **Part2** Logistic Regression\n",
        "* Understand and Implement Logistic Regression using numpy library.\n",
        "* Apply your own classifier implemented by numpy to MNIST and CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoX9ezGPY-8i"
      },
      "source": [
        "### Step1. Implement Logistic Regression using Numpy\n",
        "\n",
        "## **TODO** : Fill in the blank with the code by referring to the following formulas below for all the **steps 1-1 ~ 1-6.** You should write your own **description of your source code** in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3jAl6UjXX9C"
      },
      "source": [
        "#### step 1-1. Implement Sigmoid Function\n",
        "\n",
        "**Sigmoid:** $f(z) = \\dfrac{1}{1+e^{-z}}$ where $z=w^T x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os-9dY15XX9C"
      },
      "source": [
        "def sigmoid(z):\n",
        "    ######################## Blank ########################\n",
        "    sig = 1/(1 + np.exp(-z))\n",
        "    #######################################################\n",
        "    return sig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIDo0Dq3XX9F"
      },
      "source": [
        "# Print out the result of sigmoid function \n",
        "\n",
        "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
        "print (\"sigmoid(4.5) = \" + str(sigmoid(4.5)))\n",
        "print (\"sigmoid(-6) = \" + str(sigmoid(-6)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3C8B4X4XX9I"
      },
      "source": [
        "#### step 1-2. Implement forward propagation\n",
        "\n",
        "__x__: Input data of size (sample_size, feature_size)\n",
        "\n",
        "__y__: target (labels)\n",
        "\n",
        "__w__: model weights\n",
        "\n",
        "__predict__: prediction for each data point $x_i$; $h(w^Tx_i)=\\dfrac{1}{1+e^{-w^T x_i}}$ \n",
        "\n",
        "__loss__: $J(w)=\\frac{1}{n}\\sum_{i=1}^n[-y_i\\text{log}\\{h(w^Tx_i)\\} - (1-y_i)\\text{log}\\{1-h(w^Tx_i)\\}]$\n",
        "\n",
        "where $n$ is number of samples and $y$ is target.\n",
        "\n",
        "**NOTE**: If you use ```np.log``` method, it is recommended to use as follows,\n",
        "\n",
        "```python\n",
        "np.log(x) -> np.log(x + eps)\n",
        "```\n",
        "where ```eps``` is small positive number used to avoid taking of almost zero value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MirCkvw6XX9I"
      },
      "source": [
        "def forward(x, y, w, eps=1e-8):\n",
        "    ######################## Blank ########################\n",
        "    predict = sigmoid(np.matmul(w, x.T))\n",
        "    n = x.shape[0]\n",
        "    loss1 = - np.matmul(y.T, np.log(predict + eps))\n",
        "    loss2 = - np.matmul((np.ones(y.shape) - y).T, np.log(np.ones(predict.shape) - predict + eps))\n",
        "    loss = (1/n) * (loss1 + loss2)\n",
        "    #######################################################\n",
        "    return predict, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVlvpPA-XX9L"
      },
      "source": [
        "#### step 1-3. Implement backward propagation\n",
        "\n",
        "__Hint:__ Refer to the following derivation.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial J(\\boldsymbol{w})}{\\partial \\boldsymbol{w}} &=-\\frac{1}{n} \\sum_{i=1}^{n}\\left[y_{i}\\left(1-h\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)\\right)-\\left(1-y_{i}\\right) h\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)\\right] \\boldsymbol{x}_{i} \\\\\n",
        "&=\\frac{1}{n} \\sum_{i=1}^{n}\\left[h\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)-y_{i}\\right] \\boldsymbol{x}_{i}\\\\\n",
        "&= \\frac{1}{n} (\\hat{Y}-Y)X\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "__grad_w__: Gradient of weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmfXvjuhXX9L"
      },
      "source": [
        "def backward(x, y, predict):\n",
        "    ######################## Blank ########################\n",
        "    n = x.shape[0]\n",
        "    grad_w = (1/n) * (np.matmul(predict - y, x))\n",
        "    #######################################################\n",
        "    return grad_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFXONtmPXX9O"
      },
      "source": [
        "#### step 1-4. Add bias unit\n",
        "\n",
        "To implement the bias unit, insert ones column vertor into dataset $x$; Define $\\bar{x}$ and $\\bar{w}$ as follows:\n",
        "\n",
        "$$\n",
        "\\bar{x}:=\\left[\\begin{array}{cc}\n",
        "x_{1}^{T} & 1 \\\\\n",
        "x_{2}^{T} & 1 \\\\\n",
        "\\vdots & \\\\\n",
        "x_{n}^{T} & 1\n",
        "\\end{array}\\right] \\in \\mathbf{R}^{n \\times(d+1)}, \\bar{w}:=\\left[\\begin{array}{c}\n",
        "w \\\\\n",
        "b\n",
        "\\end{array}\\right] \\in \\mathbf{R}^{d+1}\n",
        "$$\n",
        "\n",
        "Argument __b__ of ```bias_unit``` function below refers to the bias term.\n",
        "\n",
        "__x_bar__: $\\bar{x}$\n",
        "\n",
        "__w_bar__: $\\bar{w}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xby4JRyIXX9O"
      },
      "source": [
        "def bias_unit(x, w, b):\n",
        "    ######################## Blank ########################\n",
        "    n = x.shape[0]\n",
        "    ones = np.ones((n,1))\n",
        "    x_bar = np.c_[x, ones]\n",
        "    w_bar = np.r_[w, b]\n",
        "    #######################################################\n",
        "    return x_bar, w_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH1Ms_iwXX9R"
      },
      "source": [
        "#### step 1-5. Model initialization\n",
        "\n",
        "Initialize the model(__w__ and __b__) with arbitrary(e.g. Guassian distribution) values.\n",
        "\n",
        "* Initialize __w__ with gaussian distribution (use np.random.normal)\n",
        "* Initialize __b__ with zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3mSiaMFXX9S"
      },
      "source": [
        "def initialize_params(X_train, verbose=False):\n",
        "    \n",
        "    ######################## Blank ########################\n",
        "    w = np.random.normal(0, 1, X_train.shape[1])\n",
        "    b = 0\n",
        "    #######################################################\n",
        "    \n",
        "    X_train_bar, w_bar = bias_unit(X_train, w, b) # add bias unit \n",
        "    if verbose:\n",
        "        print('Before adding the bias unit')\n",
        "        print('shape of X_train:, ', X_train.shape)\n",
        "        print('w: ', w.__repr__())\n",
        "        print('b: ', b.__repr__(), end='\\n\\n')\n",
        "        print('After adding the bias unit')\n",
        "        print('shape of X_train_bar:, ', X_train_bar.shape)\n",
        "        print('w_bar: ', w_bar.__repr__())\n",
        "    \n",
        "    return X_train_bar, w_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc52Uw7FXX9U"
      },
      "source": [
        "#### step 1-6. Implement ```accuracy``` function  for computing the accuracy\n",
        "\n",
        "Implement a function that return accuracy when ```predict``` $h(w^Tx_i)=\\frac{1}\n",
        "{1+e^{-w^T x_i}}$ and ```y``` are given.\n",
        "\n",
        "__acc__: $\\dfrac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}_{[\\hat{y}_i=y_i]} \\times 100 (\\%)$ where $\\hat{y}_i = \\mathbf{1}_{[h(w^Tx_i)\\geq0.5]}$. And $\\mathbf{1}_{A}$ is defined as:\n",
        "\n",
        "$$\\mathbf{1}_{A}:=\\left\\{\\begin{array}{ll}\n",
        "1 & \\text { if } A \\text{ is true}\\\\\n",
        "0 & \\text { if } A \\text{ is false}\n",
        "\\end{array}\\right.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb1QsEdXXX9V"
      },
      "source": [
        "def accuracy(predict, y):\n",
        "    ######################## Blank ########################\n",
        "    n = predict.size\n",
        "    pred_label = 2 * (predict >= 0.5) - 1\n",
        "    inner_product = np.matmul(pred_label.T, 2 * y - 1)\n",
        "    acc = 100 * ( inner_product + n)/(2 * n)\n",
        "    #######################################################\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1JYMhYjXX9X"
      },
      "source": [
        "### Step 2 Apply to the MNIST Dataset  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1pJwUh3Z-RH"
      },
      "source": [
        "#### step 2-1 Implement training code combining all the methods above\n",
        "\n",
        "## **TODO**: Fill in the blank using all the functions above and **attach the result** in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU7hq--tXX9b"
      },
      "source": [
        "# hyperparameter\n",
        "num_of_iteration = 5000\n",
        "learning_rate = 0.5\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(100)\n",
        "\n",
        "# Model initialization and add bias unit\n",
        "X_train_bar, w_bar = initialize_params(X_train_std, False)\n",
        "\n",
        "if w_bar.ndim != Y_train.ndim:  # if w is implemented as a column vector\n",
        "    w_bar = w_bar.squeeze()\n",
        "    \n",
        "it = tqdm(range(num_of_iteration))\n",
        "for i in it:\n",
        "    \n",
        "    ######################## Blank ########################\n",
        "    predict, loss = forward(X_train_bar, Y_train, w_bar, eps=1e-8)\n",
        "    train_acc = accuracy(predict, Y_train)\n",
        "    gradient = backward(X_train_bar, Y_train, predict)\n",
        "    w_bar = w_bar - learning_rate * gradient\n",
        "    #######################################################\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        it.set_postfix(accuracy='{:.2f}'.format(train_acc),\n",
        "                      loss='{:.4f}'.format(loss))\n",
        "    \n",
        "# compute the train & test accuracy\n",
        "predict, _ = forward(X_train_bar, Y_train, w_bar)\n",
        "train_acc = accuracy(predict, Y_train)\n",
        "print('train accuracy: {:.2f}'.format(train_acc))\n",
        "\n",
        "unit_ones = np.ones((len(X_test_std), 1))\n",
        "X_test_bar = np.concatenate((X_test_std, unit_ones), axis=1)  # add bias unit for test dataset\n",
        "\n",
        "predict, _ = forward(X_test_bar, Y_test, w_bar)\n",
        "test_acc = accuracy(predict, Y_test)\n",
        "print('test accuracy: {:.2f}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvY2Ji_daQXg"
      },
      "source": [
        "# Prediction Results\n",
        "np.random.seed(2020)\n",
        "idxs = np.random.choice(len(Y_test), 10, replace=False)\n",
        "label_to_class = {0: '4', 1: '9'}\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "for i, idx in enumerate(idxs):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    predict, _ = forward(X_test_bar, Y_test, w_bar)\n",
        "    pred_label = (predict[idx] >= 0.5).astype(np.int)\n",
        "    plt.imshow(X_test[idx], cmap='gray_r')\n",
        "    plt.title('Digit Prediction: {} '.format(label_to_class[pred_label]), fontsize=14)\n",
        "    plt.xticks([]); plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLHgL71_XX9o"
      },
      "source": [
        "### Step 3 Apply to the CIFAR10 dataset\n",
        "\n",
        "#### step 3-1 Implement training code combining all the methods above\n",
        "\n",
        "## **TODO**: Fill in the blank and **attach the result** in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc2TmggYzCT9"
      },
      "source": [
        "X_train_flatten, Y_train, X_test_flatten, Y_test = cifar10_subset([0, 1])\n",
        "\n",
        "X_train_flatten = X_train_flatten.reshape(2 * 5000, -1)\n",
        "X_test_flatten = X_test_flatten.reshape(2 * 1000, -1)\n",
        "Y_train = Y_train.reshape(-1)\n",
        "Y_test = Y_test.reshape(-1)\n",
        "\n",
        "X_train_std = X_train_flatten / 255.\n",
        "X_test_std = X_test_flatten / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtFGeNy2zwJJ"
      },
      "source": [
        "#show image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image1 = X_train_flatten[1].reshape(3,32,32).transpose([1,2,0])\n",
        "image2 = X_train_flatten[5000].reshape(3,32,32).transpose([1,2,0])\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(121)\n",
        "plt.imshow(image1,  interpolation=\"bicubic\")\n",
        "plt.title('class {}'.format(Y_train[0].astype(np.int)))\n",
        "plt.subplot(122)\n",
        "plt.imshow(image2,  interpolation=\"bicubic\")\n",
        "plt.title('class {}'.format(Y_train[5000].astype(np.int)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCtTbl5TXX9t"
      },
      "source": [
        "# hyperparameter\n",
        "num_of_iteration = 5000\n",
        "learning_rate = 0.1\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(100)\n",
        "\n",
        "# Model initialization and add bias unit\n",
        "X_train_bar, w_bar = initialize_params(X_train_std, False)\n",
        "\n",
        "if w_bar.ndim != Y_train.ndim:  # if w is implemented as a column vector\n",
        "    w_bar = w_bar.squeeze()\n",
        "    \n",
        "it = tqdm(range(num_of_iteration))\n",
        "for i in it:\n",
        "    \n",
        "    ######################## Blank ########################\n",
        "    predict, loss = forward(X_train_bar, Y_train, w_bar, eps=1e-8) \n",
        "    train_acc = accuracy(predict, Y_train)\n",
        "    gradient = backward(X_train_bar, Y_train, predict)\n",
        "    w_bar = w_bar - learning_rate * gradient\n",
        "    #######################################################\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        it.set_postfix(accuracy='{:.2f}'.format(train_acc),\n",
        "                      loss='{:.4f}'.format(loss))\n",
        "    \n",
        "# compute the train & test accuracy\n",
        "predict, _ = forward(X_train_bar, Y_train, w_bar)\n",
        "train_acc = accuracy(predict, Y_train)\n",
        "print('train accuracy: {:.2f}'.format(train_acc))\n",
        "\n",
        "unit_ones = np.ones((len(X_test_std), 1))\n",
        "X_test_bar = np.concatenate((X_test_std, unit_ones), axis=1)  # add bias unit for test dataset\n",
        "\n",
        "predict, _ = forward(X_test_bar, Y_test, w_bar)\n",
        "test_acc = accuracy(predict, Y_test)\n",
        "print('test accuracy: {:.2f}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MofuBcEgaSUm"
      },
      "source": [
        "### Step 4 Implement Logistic Regression with L2 Regularization\n",
        "\n",
        "## **TODO** : Fill in the blank with the code by referring to the following formulas below for all the **steps 4-1 ~ 4-2.** You should write your own **description of your source code** in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwJIcJh9aZuS"
      },
      "source": [
        "#### step 4-1. Implement forward propagation with regularization term\n",
        "\n",
        "__x__: Input data of size (sample_size, feature_size); $(x_1, y_1)$, $(x_2, y_2)$, ... $(x_n, y_n)$.\n",
        "\n",
        "__y__: target (labels)\n",
        "\n",
        "__w__: model weights\n",
        "\n",
        "**lambda_**: regularization parameter\n",
        "\n",
        "__predict__: Prediction for each data point $x_i$ : $h(w^Tx_i)=\\dfrac{1}{1+e^{-w^T x_i}}$ \n",
        "\n",
        "__loss__: $J(w)=\\frac{1}{n}\\sum_{i=1}^n[-y_i\\text{log}\\{h(w^Tx_i)\\} - (1-y_i)\\text{log}\\{1-h(w^Tx_i)\\}] + \\frac{\\lambda}{2n}\\Vert w\\Vert^2_2$\n",
        "\n",
        "where $n$ is number of samples and $y$ is target.\n",
        "\n",
        "**Hint**: use ```numpy.linalg.norm``` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfqqrWetXX9g"
      },
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "def forward_with_regularization(x, y, w, lambda_, eps=1e-8):\n",
        "    ######################## Blank ########################\n",
        "    predict = sigmoid(np.matmul(w, x.T))\n",
        "    n = x.shape[0]\n",
        "    loss1 = - np.matmul(y.T, np.log(predict + eps))\n",
        "    loss2 = - np.matmul((np.ones(y.shape) - y).T, np.log(np.ones(predict.shape) - predict + eps))\n",
        "    loss = (1/x.shape[0]) * (loss1 + loss2 + (lambda_/2) * (np.linalg.norm(w) ** 2))\n",
        "    #######################################################\n",
        "    return predict, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP9KSKmlXX9j"
      },
      "source": [
        "#### step 4-2. Implement backward propagation\n",
        "\n",
        "__Hint:__ Refer to the following derivation.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial J(\\boldsymbol{w})}{\\partial \\boldsymbol{w}} &=-\\frac{1}{n} \\sum_{i=1}^{n}\\left[y_{i}\\left(1-h\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)\\right)-\\left(1-y_{i}\\right) h\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)\\right] \\boldsymbol{x}_{i} + \\frac{\\lambda}{n}w \\\\\n",
        "&=\\frac{1}{n} \\sum_{i=1}^{n}\\left[h\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)-y_{i}\\right] \\boldsymbol{x}_{i} + \\frac{\\lambda}{n}w\\\\\n",
        "&= \\frac{1}{n} (\\hat{Y}-Y)X + \\frac{\\lambda}{n}w\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IvUmlq4XX9j"
      },
      "source": [
        "def backward_with_regularization(x, y, w, predict, lambda_):\n",
        "    ######################## Blank ########################\n",
        "    n = x.shape[0]\n",
        "    grad_w = (1/n) * (np.matmul(predict - y, x) + lambda_ * w)\n",
        "    #######################################################\n",
        "    return grad_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEP-15_2XX9l"
      },
      "source": [
        "#### step 4-3. Training Regularized logistic regression with different $\\lambda$ (Apply to the MNIST dataset)\n",
        "\n",
        "\n",
        "\n",
        "Load the MNIST dataset again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ObYTaNy04CR"
      },
      "source": [
        "# Dataset Load\n",
        "X_train = np.load('mnist_49/X_train.npy')\n",
        "X_test = np.load('mnist_49/X_test.npy')\n",
        "Y_train = np.load('mnist_49/Y_train.npy')\n",
        "Y_test = np.load('mnist_49/Y_test.npy')\n",
        "\n",
        "# Reshape the training and test examples\n",
        "# Data from Part 1 - Step 1\n",
        "X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "X_train_std = X_train_flatten / 255.\n",
        "X_test_std = X_test_flatten / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oKSNDGc1K9V"
      },
      "source": [
        "## **TODO**: Fill in the blank and **attach the results and the figure** in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8vJ7cNVXX9m"
      },
      "source": [
        "# hyperparameter\n",
        "num_of_iteration = 5000\n",
        "learning_rate = 0.5\n",
        "lin_lambdas = np.linspace(-2, 3, 10)  \n",
        "lambdas = np.power(np.e, lin_lambdas)  # Regularization parameter (lambda) = e^-2 ~ e^3\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(100)\n",
        "\n",
        "# add bias unit for test dataset\n",
        "unit_ones = np.ones((len(X_test_std), 1))\n",
        "X_test_bar = np.concatenate((X_test_std, unit_ones), axis=1)  \n",
        "\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "    \n",
        "for lambda_ in lambdas:\n",
        "\n",
        "    # Model initialization and add bias unit\n",
        "    X_train_bar, w_bar = initialize_params(X_train_std, False)\n",
        "\n",
        "    if w_bar.ndim != Y_train.ndim:  # if w is implemented as a column vector\n",
        "        w_bar = w_bar.squeeze()\n",
        "    \n",
        "    it = tqdm(range(num_of_iteration))\n",
        "    for i in it:\n",
        "        \n",
        "        ######################## Blank ########################\n",
        "        predict, loss = forward_with_regularization(X_train_bar, Y_train, w_bar, lambda_, eps=1e-8)\n",
        "        train_acc = accuracy(predict, Y_train)\n",
        "        gradient = backward_with_regularization(X_train_bar, Y_train, w_bar, predict, lambda_)\n",
        "        w_bar = w_bar - learning_rate * gradient\n",
        "        #######################################################\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            it.set_postfix(accuracy='{:.2f}'.format(train_acc),\n",
        "                          loss='{:.4f}'.format(loss))\n",
        "    \n",
        "    # compute the train & test accuracy\n",
        "    predict, _ = forward(X_train_bar, Y_train, w_bar)\n",
        "    train_acc = accuracy(predict, Y_train)\n",
        "    print('train accuracy: {:.2f}'.format(train_acc))\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    predict, _ = forward(X_test_bar, Y_test, w_bar)\n",
        "    test_acc = accuracy(predict, Y_test)\n",
        "    print('test accuracy: {:.2f}'.format(test_acc))\n",
        "    test_accuracies.append(test_acc)\n",
        "    \n",
        "plt.plot(lin_lambdas, train_accuracies, label='train acc')\n",
        "plt.plot(lin_lambdas, test_accuracies, label='test acc')\n",
        "plt.xlabel('$\\ln\\lambda$')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOTUuco34VOe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}